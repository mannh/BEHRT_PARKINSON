{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from common.common import create_folder,H5Recorder\n",
    "from common.conf_loader import ConfLoader\n",
    "import configs.config_classes as conf_objs\n",
    "from configs.config_classes import BertConfig\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "\n",
    "from model import optimiser\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from common.common import load_obj\n",
    "from model.utils import age_vocab\n",
    "from dataLoader.NextXVisit import NextVisit\n",
    "from model.NextXVisit import BertForMultiLabelPrediction\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "conf_loader = ConfLoader('../configs/proact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_parquet(f'../data/for_behrt_wrt_months_more_than_3_months_fixed.parquet')#.iloc[:2536,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    PID                                               data  \\\n0   329  [CLS, Q1_Speech_score_4, Q2_Salivation_score_3...   \n1   533  [CLS, Q1_Speech_score_1, Q2_Salivation_score_2...   \n2   649  [CLS, Q1_Speech_score_3, Q2_Salivation_score_3...   \n3   708  [CLS, Q1_Speech_score_3, Q2_Salivation_score_3...   \n4  1234  [CLS, Q1_Speech_score_2, Q2_Salivation_score_3...   \n\n                                                rank  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n1               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n\n                                                 age  \\\n0  [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...   \n1   [65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65]   \n2  [48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...   \n3  [46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 4...   \n4  [38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...   \n\n                                        ALSFRS_Total  \\\n0  [27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 2...   \n1   [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]   \n2  [36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 3...   \n3  [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 3...   \n4  [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2...   \n\n                                                time  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 42, 4...   \n1               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 32, 3...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 35, 3...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 1...   \n\n                                                 FVC  \\\n0  [0.5294117647058824, 0.5294117647058824, 0.529...   \n1  [0.029411764705882353, 0.029411764705882353, 0...   \n2  [1.1764705882352942, 1.1764705882352942, 1.176...   \n3  [-0.8235294117647058, -0.8235294117647058, -0....   \n4  [-0.5588235294117647, -0.5588235294117647, -0....   \n\n                                                  CK  \\\n0  [0.5271317829457365, 0.5271317829457365, 0.527...   \n1  [-0.37209302325581395, -0.37209302325581395, -...   \n2  [-0.5697674418604651, -0.5697674418604651, -0....   \n3  [-0.5271317829457365, -0.5271317829457365, -0....   \n4  [0.003875968992248062, 0.003875968992248062, 0...   \n\n                                          Creatinine  \\\n0  [0.5005656108597285, 0.5005656108597285, 0.500...   \n1  [0.25056561085972845, 0.25056561085972845, 0.2...   \n2  [-0.4994343891402715, -0.4994343891402715, -0....   \n3  [0.5005656108597285, 0.5005656108597285, 0.500...   \n4  [0.5005656108597285, 0.5005656108597285, 0.500...   \n\n                                          Phosphorus  ...  \\\n0  [0.3333333333333326, 0.3333333333333326, 0.333...  ...   \n1  [-2.0000000000000027, -2.0000000000000027, -2....  ...   \n2  [-1.3333333333333355, -1.3333333333333355, -1....  ...   \n3  [-1.3333333333333355, -1.3333333333333355, -1....  ...   \n4  [1.166666666666668, 1.166666666666668, 1.16666...  ...   \n\n                                           Potassium  \\\n0  [-0.2499999999999989, -0.2499999999999989, -0....   \n1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n2  [0.2500000000000011, 0.2500000000000011, 0.250...   \n3  [2.249999999999999, 2.249999999999999, 2.24999...   \n4  [-0.2499999999999989, -0.2499999999999989, -0....   \n\n                                             Protein  \\\n0  [-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0....   \n1  [-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0....   \n2  [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, ...   \n3  [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, ...   \n4  [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, ...   \n\n                                             Glucose  \\\n0  [-0.3277272727272726, -0.3277272727272726, -0....   \n1  [1.9427272727272722, 1.9427272727272722, 1.942...   \n2  [-0.42863636363636326, -0.42863636363636326, -...   \n3  [-0.3277272727272726, -0.3277272727272726, -0....   \n4  [-0.17636363636363622, -0.17636363636363622, -...   \n\n                                             Calcium  \\\n0  [-0.484466019417476, -0.484466019417476, -0.48...   \n1  [-0.6459546925566347, -0.6459546925566347, -0....   \n2  [-0.9689320388349492, -0.9689320388349492, -0....   \n3  [1.1304207119741108, 1.1304207119741108, 1.130...   \n4  [0.32297734627831737, 0.32297734627831737, 0.3...   \n\n                                              Sodium  \\\n0  [-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0....   \n1  [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3  [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.2...   \n4  [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...   \n\n                                           Platelets  \\\n0  [0.49333333333333335, 0.49333333333333335, 0.4...   \n1  [-0.8933333333333333, -0.8933333333333333, -0....   \n2  [0.38666666666666666, 0.38666666666666666, 0.3...   \n3  [0.013333333333333334, 0.013333333333333334, 0...   \n4  [-0.28, -0.28, -0.28, -0.28, -0.28, -0.28, -0....   \n\n                                          fixed_time  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 42, 4...   \n1               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 32, 3...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 35, 3...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 1...   \n\n                                        fixed_months 3_months_idx  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           33   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           11   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           44   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           33   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           44   \n\n                                   FRS_diag_original  \n0  [CLS, Q1_Speech_score_4, Q2_Salivation_score_3...  \n1  [CLS, Q1_Speech_score_1, Q2_Salivation_score_2...  \n2  [CLS, Q1_Speech_score_3, Q2_Salivation_score_3...  \n3  [CLS, Q1_Speech_score_3, Q2_Salivation_score_3...  \n4  [CLS, Q1_Speech_score_2, Q2_Salivation_score_3...  \n\n[5 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>data</th>\n      <th>rank</th>\n      <th>age</th>\n      <th>ALSFRS_Total</th>\n      <th>time</th>\n      <th>FVC</th>\n      <th>CK</th>\n      <th>Creatinine</th>\n      <th>Phosphorus</th>\n      <th>...</th>\n      <th>Potassium</th>\n      <th>Protein</th>\n      <th>Glucose</th>\n      <th>Calcium</th>\n      <th>Sodium</th>\n      <th>Platelets</th>\n      <th>fixed_time</th>\n      <th>fixed_months</th>\n      <th>3_months_idx</th>\n      <th>FRS_diag_original</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>329</td>\n      <td>[CLS, Q1_Speech_score_4, Q2_Salivation_score_3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...</td>\n      <td>[27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 42, 4...</td>\n      <td>[0.5294117647058824, 0.5294117647058824, 0.529...</td>\n      <td>[0.5271317829457365, 0.5271317829457365, 0.527...</td>\n      <td>[0.5005656108597285, 0.5005656108597285, 0.500...</td>\n      <td>[0.3333333333333326, 0.3333333333333326, 0.333...</td>\n      <td>...</td>\n      <td>[-0.2499999999999989, -0.2499999999999989, -0....</td>\n      <td>[-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0....</td>\n      <td>[-0.3277272727272726, -0.3277272727272726, -0....</td>\n      <td>[-0.484466019417476, -0.484466019417476, -0.48...</td>\n      <td>[-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0....</td>\n      <td>[0.49333333333333335, 0.49333333333333335, 0.4...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 42, 4...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>33</td>\n      <td>[CLS, Q1_Speech_score_4, Q2_Salivation_score_3...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>533</td>\n      <td>[CLS, Q1_Speech_score_1, Q2_Salivation_score_2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65]</td>\n      <td>[30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[0.029411764705882353, 0.029411764705882353, 0...</td>\n      <td>[-0.37209302325581395, -0.37209302325581395, -...</td>\n      <td>[0.25056561085972845, 0.25056561085972845, 0.2...</td>\n      <td>[-2.0000000000000027, -2.0000000000000027, -2....</td>\n      <td>...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n      <td>[-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0....</td>\n      <td>[1.9427272727272722, 1.9427272727272722, 1.942...</td>\n      <td>[-0.6459546925566347, -0.6459546925566347, -0....</td>\n      <td>[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n      <td>[-0.8933333333333333, -0.8933333333333333, -0....</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>11</td>\n      <td>[CLS, Q1_Speech_score_1, Q2_Salivation_score_2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>649</td>\n      <td>[CLS, Q1_Speech_score_3, Q2_Salivation_score_3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n      <td>[48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...</td>\n      <td>[36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 32, 3...</td>\n      <td>[1.1764705882352942, 1.1764705882352942, 1.176...</td>\n      <td>[-0.5697674418604651, -0.5697674418604651, -0....</td>\n      <td>[-0.4994343891402715, -0.4994343891402715, -0....</td>\n      <td>[-1.3333333333333355, -1.3333333333333355, -1....</td>\n      <td>...</td>\n      <td>[0.2500000000000011, 0.2500000000000011, 0.250...</td>\n      <td>[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, ...</td>\n      <td>[-0.42863636363636326, -0.42863636363636326, -...</td>\n      <td>[-0.9689320388349492, -0.9689320388349492, -0....</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.38666666666666666, 0.38666666666666666, 0.3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 32, 3...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>44</td>\n      <td>[CLS, Q1_Speech_score_3, Q2_Salivation_score_3...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>708</td>\n      <td>[CLS, Q1_Speech_score_3, Q2_Salivation_score_3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n      <td>[46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 4...</td>\n      <td>[33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 35, 3...</td>\n      <td>[-0.8235294117647058, -0.8235294117647058, -0....</td>\n      <td>[-0.5271317829457365, -0.5271317829457365, -0....</td>\n      <td>[0.5005656108597285, 0.5005656108597285, 0.500...</td>\n      <td>[-1.3333333333333355, -1.3333333333333355, -1....</td>\n      <td>...</td>\n      <td>[2.249999999999999, 2.249999999999999, 2.24999...</td>\n      <td>[1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, ...</td>\n      <td>[-0.3277272727272726, -0.3277272727272726, -0....</td>\n      <td>[1.1304207119741108, 1.1304207119741108, 1.130...</td>\n      <td>[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.2...</td>\n      <td>[0.013333333333333334, 0.013333333333333334, 0...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 35, 3...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>33</td>\n      <td>[CLS, Q1_Speech_score_3, Q2_Salivation_score_3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1234</td>\n      <td>[CLS, Q1_Speech_score_2, Q2_Salivation_score_3...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n      <td>[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 3...</td>\n      <td>[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 1...</td>\n      <td>[-0.5588235294117647, -0.5588235294117647, -0....</td>\n      <td>[0.003875968992248062, 0.003875968992248062, 0...</td>\n      <td>[0.5005656108597285, 0.5005656108597285, 0.500...</td>\n      <td>[1.166666666666668, 1.166666666666668, 1.16666...</td>\n      <td>...</td>\n      <td>[-0.2499999999999989, -0.2499999999999989, -0....</td>\n      <td>[1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, ...</td>\n      <td>[-0.17636363636363622, -0.17636363636363622, -...</td>\n      <td>[0.32297734627831737, 0.32297734627831737, 0.3...</td>\n      <td>[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7...</td>\n      <td>[-0.28, -0.28, -0.28, -0.28, -0.28, -0.28, -0....</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 1...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>44</td>\n      <td>[CLS, Q1_Speech_score_2, Q2_Salivation_score_3...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = temp.rename(columns={'ID':'PID', 'FRS_diag':'data'})\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lab_test = 'creatinine'\n",
    "# preper target\n",
    "def get_days(months):\n",
    "    return (months / 12) * 365.24\n",
    "dt = 12\n",
    "valid_for_train_ub =  get_days(3 + dt) + 7\n",
    "valid_for_train_lb =  get_days(3 + dt) - 7\n",
    "def get_last_valid_idx(ls):\n",
    "    s = pd.Series([valid_for_train_lb <= i <= valid_for_train_ub  for i in ls])\n",
    "    return s.where(s).last_valid_index()\n",
    "temp['target_idx'] = temp['fixed_time'].apply(get_last_valid_idx)\n",
    "\n",
    "train = temp.iloc[:2536,:].reset_index()\n",
    "test = temp.iloc[2536:, :].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "train=train.loc[~train['target_idx'].isna(),:].reset_index()\n",
    "test=test.loc[~test['target_idx'].isna(),:].reset_index()\n",
    "train['target_idx']=train['target_idx'].astype(int)\n",
    "test['target_idx']=test['target_idx'].astype(int)\n",
    "# temp.apply(lambda x: x['FRS_diag_original'][:x['target_idx']+1], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['target'] = train.apply(lambda x: x['FRS_diag_original'][x['target_idx']-10:x['target_idx']], axis=1)\n",
    "test['target'] = test.apply(lambda x: x['FRS_diag_original'][x['target_idx']-10:x['target_idx']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "\n",
    "train_s = train.iloc[:int(train.shape[0]*.8),].reset_index(drop=True)\n",
    "validation = train.iloc[int(train.shape[0]*.8):,].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "n_train = train_s.shape[0]\n",
    "n_validation = validation.shape[0]\n",
    "n_test = test.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BEST_MODEL_NAME = f'best_als_visit_next_{dt}_months_predictor_{lab_test}_small'\n",
    "file_config = conf_loader.load_configs('file_config_next_visit.json')\n",
    "global_params = conf_loader.load_configs('global_params.json')\n",
    "optim_config = conf_loader.load_configs('optimizer_params.json')\n",
    "file_config['saved_model_name'] = BEST_MODEL_NAME\n",
    "pretrain_model_path = 'model_path/proact_model_visits_creatinine_small\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train.to_parquet(file_config.get('train_all'))\n",
    "train_s.to_parquet(file_config.get('train'))\n",
    "validation.to_parquet(file_config.get('validation'))\n",
    "test.to_parquet(file_config.get('test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], symbol=global_params['age_symbol'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageVocab = {k:v for k,v in ageVocab.items() if v <=85 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-format label token\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    del token2idx['PAD']\n",
    "    del token2idx['SEP']\n",
    "    del token2idx['CLS']\n",
    "    del token2idx['MASK']\n",
    "    token = list(token2idx.keys())\n",
    "    labelVocab = {}\n",
    "    for i,x in enumerate(token):\n",
    "        labelVocab[x] = i\n",
    "    return labelVocab\n",
    "\n",
    "labelVocab = format_label_vocab(BertVocab['token2idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = conf_loader.load_model_configs('model_params.json', global_params, BertVocab, ageVocab)\n",
    "\n",
    "feature_dict = {\n",
    "    'word':True,\n",
    "    'seg':True,\n",
    "    'age':True,\n",
    "    'position': True,\n",
    "    'creatinine':True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, num_workers=3):\n",
    "    Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=df, max_len=global_params['max_len_seq'],\n",
    "                 code='data', label='target', patid='PID')\n",
    "    return DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "train = pd.read_parquet(file_config['train'])\n",
    "test = pd.read_parquet(file_config['test'])\n",
    "\n",
    "train_load = get_loader(train)\n",
    "test_load = get_loader(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, num_labels=len(labelVocab.keys()), feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = model.to(global_params['device'])\n",
    "optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(train_load):\n",
    "        cnt +=1\n",
    "        age_ids, input_ids, posi_ids, creatinine, segment_ids, attMask, targets, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        creatinine = creatinine.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,creatinine,attention_mask=attMask, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 50==0:\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t\".format(e, cnt,temp_loss/50))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_load):\n",
    "        model.eval()\n",
    "        age_ids, input_ids, posi_ids, creatinine, segment_ids, attMask, targets, _ = batch\n",
    "        \n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        creatinine = creatinine.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,creatinine,attention_mask=attMask, labels=targets)\n",
    "        logits = [logit.cpu() for logit in logits]\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    # y_label = torch.cat(y_label, dim=0)\n",
    "    # y = torch.cat(y, dim=0)\n",
    "\n",
    "    # aps, roc, output, label = precision_test(y, y_label)\n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Cnt: 1\t| Loss: 0.013345861434936523\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.6359679698944092 \n",
      "epoch: 1\t| Cnt: 1\t| Loss: 0.011321872472763062\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5789742469787598 \n",
      "epoch: 2\t| Cnt: 1\t| Loss: 0.010647249221801759\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.547497034072876 \n",
      "epoch: 3\t| Cnt: 1\t| Loss: 0.010360623598098756\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5335565209388733 \n",
      "epoch: 4\t| Cnt: 1\t| Loss: 0.008559824228286743\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5291281938552856 \n",
      "epoch: 5\t| Cnt: 1\t| Loss: 0.008054086565971374\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5273545384407043 \n",
      "epoch: 6\t| Cnt: 1\t| Loss: 0.006739607453346253\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5262475609779358 \n",
      "epoch: 7\t| Cnt: 1\t| Loss: 0.009805093407630921\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5254848003387451 \n",
      "epoch: 8\t| Cnt: 1\t| Loss: 0.007313929796218872\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5246336460113525 \n",
      "epoch: 9\t| Cnt: 1\t| Loss: 0.00826500952243805\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5227473974227905 \n",
      "epoch: 10\t| Cnt: 1\t| Loss: 0.007103093266487122\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5202760100364685 \n",
      "epoch: 11\t| Cnt: 1\t| Loss: 0.006897004246711731\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5166970491409302 \n",
      "epoch: 12\t| Cnt: 1\t| Loss: 0.007224380970001221\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5125051736831665 \n",
      "epoch: 13\t| Cnt: 1\t| Loss: 0.007815867066383361\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.507714569568634 \n",
      "epoch: 14\t| Cnt: 1\t| Loss: 0.007662972211837768\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5040268898010254 \n",
      "epoch: 15\t| Cnt: 1\t| Loss: 0.006917705535888672\t\n",
      " | test loss : 0.5051828622817993 \n",
      "epoch: 16\t| Cnt: 1\t| Loss: 0.005896261334419251\t\n",
      " | test loss : 0.5136426687240601 \n",
      "epoch: 17\t| Cnt: 1\t| Loss: 0.005466774106025696\t\n",
      " | test loss : 0.5256277918815613 \n",
      "epoch: 18\t| Cnt: 1\t| Loss: 0.006354843378067017\t\n",
      " | test loss : 0.5391880869865417 \n",
      "epoch: 19\t| Cnt: 1\t| Loss: 0.006252725124359131\t\n",
      " | test loss : 0.5467201471328735 \n",
      "epoch: 20\t| Cnt: 1\t| Loss: 0.006067296266555786\t\n",
      " | test loss : 0.5484057068824768 \n",
      "epoch: 21\t| Cnt: 1\t| Loss: 0.005604552626609802\t\n",
      " | test loss : 0.5494294166564941 \n",
      "epoch: 22\t| Cnt: 1\t| Loss: 0.005359163284301758\t\n",
      " | test loss : 0.5494327545166016 \n",
      "epoch: 23\t| Cnt: 1\t| Loss: 0.006222776174545288\t\n",
      " | test loss : 0.5495478510856628 \n",
      "epoch: 24\t| Cnt: 1\t| Loss: 0.0061293673515319825\t\n",
      " | test loss : 0.548612654209137 \n",
      "epoch: 25\t| Cnt: 1\t| Loss: 0.004811142683029175\t\n",
      " | test loss : 0.5470448732376099 \n",
      "epoch: 26\t| Cnt: 1\t| Loss: 0.004927724599838257\t\n",
      " | test loss : 0.5464425683021545 \n",
      "epoch: 27\t| Cnt: 1\t| Loss: 0.005065903663635254\t\n",
      " | test loss : 0.5443530678749084 \n",
      "epoch: 28\t| Cnt: 1\t| Loss: 0.005392296314239502\t\n",
      " | test loss : 0.5409737229347229 \n",
      "epoch: 29\t| Cnt: 1\t| Loss: 0.004868437647819519\t\n",
      " | test loss : 0.5397129058837891 \n",
      "epoch: 30\t| Cnt: 1\t| Loss: 0.003675207793712616\t\n",
      " | test loss : 0.5373629331588745 \n",
      "epoch: 31\t| Cnt: 1\t| Loss: 0.004856504201889038\t\n",
      " | test loss : 0.5347155332565308 \n",
      "epoch: 32\t| Cnt: 1\t| Loss: 0.004781898260116577\t\n",
      " | test loss : 0.5326815247535706 \n",
      "epoch: 33\t| Cnt: 1\t| Loss: 0.00432387888431549\t\n",
      " | test loss : 0.5300595164299011 \n",
      "epoch: 34\t| Cnt: 1\t| Loss: 0.00399195671081543\t\n",
      " | test loss : 0.5293176770210266 \n",
      "epoch: 35\t| Cnt: 1\t| Loss: 0.003147197663784027\t\n",
      " | test loss : 0.5293327569961548 \n",
      "epoch: 36\t| Cnt: 1\t| Loss: 0.0035221558809280396\t\n",
      " | test loss : 0.5292603969573975 \n",
      "epoch: 37\t| Cnt: 1\t| Loss: 0.003283291757106781\t\n",
      " | test loss : 0.5261483788490295 \n",
      "epoch: 38\t| Cnt: 1\t| Loss: 0.00430759847164154\t\n",
      " | test loss : 0.5256309509277344 \n",
      "epoch: 39\t| Cnt: 1\t| Loss: 0.004047173261642456\t\n",
      " | test loss : 0.5264581441879272 \n",
      "epoch: 40\t| Cnt: 1\t| Loss: 0.0034564584493637085\t\n",
      " | test loss : 0.5286579132080078 \n",
      "epoch: 41\t| Cnt: 1\t| Loss: 0.0033624035120010374\t\n",
      " | test loss : 0.5286509990692139 \n",
      "epoch: 42\t| Cnt: 1\t| Loss: 0.0034566447138786316\t\n",
      " | test loss : 0.52714604139328 \n",
      "epoch: 43\t| Cnt: 1\t| Loss: 0.003296943008899689\t\n",
      " | test loss : 0.5248542428016663 \n",
      "epoch: 44\t| Cnt: 1\t| Loss: 0.0023168492317199706\t\n",
      " | test loss : 0.5283185839653015 \n",
      "epoch: 45\t| Cnt: 1\t| Loss: 0.00359367311000824\t\n",
      " | test loss : 0.5322467088699341 \n",
      "epoch: 46\t| Cnt: 1\t| Loss: 0.0034365931153297422\t\n",
      " | test loss : 0.5310983061790466 \n",
      "epoch: 47\t| Cnt: 1\t| Loss: 0.003542429208755493\t\n",
      " | test loss : 0.5289439558982849 \n",
      "epoch: 48\t| Cnt: 1\t| Loss: 0.0039205402135849\t\n",
      " | test loss : 0.5294807553291321 \n",
      "epoch: 49\t| Cnt: 1\t| Loss: 0.0029676645994186402\t\n",
      " | test loss : 0.532687783241272 \n",
      "epoch: 50\t| Cnt: 1\t| Loss: 0.002883359491825104\t\n",
      " | test loss : 0.5392974019050598 \n",
      "epoch: 51\t| Cnt: 1\t| Loss: 0.0028520062565803527\t\n",
      " | test loss : 0.5425127744674683 \n",
      "epoch: 52\t| Cnt: 1\t| Loss: 0.00256857693195343\t\n",
      " | test loss : 0.5409784317016602 \n",
      "epoch: 53\t| Cnt: 1\t| Loss: 0.0028179517388343812\t\n",
      " | test loss : 0.5403904318809509 \n",
      "epoch: 54\t| Cnt: 1\t| Loss: 0.002698780298233032\t\n",
      " | test loss : 0.5410483479499817 \n",
      "epoch: 55\t| Cnt: 1\t| Loss: 0.0038970258831977846\t\n",
      " | test loss : 0.5415563583374023 \n",
      "epoch: 56\t| Cnt: 1\t| Loss: 0.003208509087562561\t\n",
      " | test loss : 0.5416094064712524 \n",
      "epoch: 57\t| Cnt: 1\t| Loss: 0.0024645593762397766\t\n",
      " | test loss : 0.5409634113311768 \n",
      "epoch: 58\t| Cnt: 1\t| Loss: 0.003110312819480896\t\n",
      " | test loss : 0.5423823595046997 \n",
      "epoch: 59\t| Cnt: 1\t| Loss: 0.0030601322650909424\t\n",
      " | test loss : 0.5417428016662598 \n",
      "epoch: 60\t| Cnt: 1\t| Loss: 0.003694139122962952\t\n",
      " | test loss : 0.5401071906089783 \n",
      "epoch: 61\t| Cnt: 1\t| Loss: 0.0027938005328178406\t\n",
      " | test loss : 0.5377111434936523 \n",
      "epoch: 62\t| Cnt: 1\t| Loss: 0.0029628762602806093\t\n",
      " | test loss : 0.538338303565979 \n",
      "epoch: 63\t| Cnt: 1\t| Loss: 0.002448720335960388\t\n",
      " | test loss : 0.5394377708435059 \n",
      "epoch: 64\t| Cnt: 1\t| Loss: 0.0025901997089385984\t\n",
      " | test loss : 0.5393116474151611 \n",
      "epoch: 65\t| Cnt: 1\t| Loss: 0.0025667122006416323\t\n",
      " | test loss : 0.5385763049125671 \n",
      "epoch: 66\t| Cnt: 1\t| Loss: 0.0025443732738494873\t\n",
      " | test loss : 0.5379559993743896 \n",
      "epoch: 67\t| Cnt: 1\t| Loss: 0.0027290260791778564\t\n",
      " | test loss : 0.5375983715057373 \n",
      "epoch: 68\t| Cnt: 1\t| Loss: 0.00322309672832489\t\n",
      " | test loss : 0.5394705533981323 \n",
      "epoch: 69\t| Cnt: 1\t| Loss: 0.002508638799190521\t\n",
      " | test loss : 0.5393658876419067 \n",
      "epoch: 70\t| Cnt: 1\t| Loss: 0.0023604625463485717\t\n",
      " | test loss : 0.5379992723464966 \n",
      "epoch: 71\t| Cnt: 1\t| Loss: 0.0024111329019069674\t\n",
      " | test loss : 0.5376052856445312 \n",
      "epoch: 72\t| Cnt: 1\t| Loss: 0.0026384973526000976\t\n",
      " | test loss : 0.536624014377594 \n",
      "epoch: 73\t| Cnt: 1\t| Loss: 0.0023505207896232605\t\n",
      " | test loss : 0.5346499085426331 \n",
      "epoch: 74\t| Cnt: 1\t| Loss: 0.00262512743473053\t\n",
      " | test loss : 0.5326900482177734 \n",
      "epoch: 75\t| Cnt: 1\t| Loss: 0.002205689549446106\t\n",
      " | test loss : 0.5293374061584473 \n",
      "epoch: 76\t| Cnt: 1\t| Loss: 0.0026823690533638\t\n",
      " | test loss : 0.5248374342918396 \n",
      "epoch: 77\t| Cnt: 1\t| Loss: 0.0031894516944885255\t\n",
      " | test loss : 0.5219870805740356 \n",
      "epoch: 78\t| Cnt: 1\t| Loss: 0.003247248828411102\t\n",
      " | test loss : 0.5210161209106445 \n",
      "epoch: 79\t| Cnt: 1\t| Loss: 0.0028750380873680116\t\n",
      " | test loss : 0.5205532312393188 \n",
      "epoch: 80\t| Cnt: 1\t| Loss: 0.0024201816320419314\t\n",
      " | test loss : 0.51902836561203 \n",
      "epoch: 81\t| Cnt: 1\t| Loss: 0.002184456437826157\t\n",
      " | test loss : 0.518345832824707 \n",
      "epoch: 82\t| Cnt: 1\t| Loss: 0.002438911646604538\t\n",
      " | test loss : 0.5180317163467407 \n",
      "epoch: 83\t| Cnt: 1\t| Loss: 0.0022939130663871764\t\n",
      " | test loss : 0.5176271796226501 \n",
      "epoch: 84\t| Cnt: 1\t| Loss: 0.0021428471803665162\t\n",
      " | test loss : 0.5130066275596619 \n",
      "epoch: 85\t| Cnt: 1\t| Loss: 0.0026841557025909426\t\n",
      " | test loss : 0.5080645680427551 \n",
      "epoch: 86\t| Cnt: 1\t| Loss: 0.0019681252539157867\t\n",
      " | test loss : 0.5045311450958252 \n",
      "epoch: 87\t| Cnt: 1\t| Loss: 0.002355968356132507\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5024138689041138 \n",
      "epoch: 88\t| Cnt: 1\t| Loss: 0.002342272996902466\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.5004890561103821 \n",
      "epoch: 89\t| Cnt: 1\t| Loss: 0.0029427048563957212\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.4985451102256775 \n",
      "epoch: 90\t| Cnt: 1\t| Loss: 0.002264126241207123\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.494638055562973 \n",
      "epoch: 91\t| Cnt: 1\t| Loss: 0.0019455444812774658\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.490966260433197 \n",
      "epoch: 92\t| Cnt: 1\t| Loss: 0.0024695909023284912\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.48954057693481445 \n",
      "epoch: 93\t| Cnt: 1\t| Loss: 0.0021649131178855894\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.4868781268596649 \n",
      "epoch: 94\t| Cnt: 1\t| Loss: 0.002203056365251541\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.4833838939666748 \n",
      "epoch: 95\t| Cnt: 1\t| Loss: 0.002348814606666565\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.47989174723625183 \n",
      "epoch: 96\t| Cnt: 1\t| Loss: 0.0027314543724060057\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.4770868718624115 \n",
      "epoch: 97\t| Cnt: 1\t| Loss: 0.0019413469731807708\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.47575104236602783 \n",
      "epoch: 98\t| Cnt: 1\t| Loss: 0.002460362762212753\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      " | test loss : 0.4747715890407562 \n",
      "epoch: 99\t| Cnt: 1\t| Loss: 0.0022890810668468476\t\n",
      " | test loss : 0.47711285948753357 \n"
     ]
    }
   ],
   "source": [
    "best_pre = np.inf\n",
    "for e in range(100):\n",
    "    train(e)\n",
    "    test_loss = evaluation()\n",
    "    if test_loss < best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(file_config['output_dir'],file_config['saved_model_name'])\n",
    "        create_folder(file_config['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = test_loss\n",
    "    print(f' | test loss : {test_loss} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(file_config['output_dir'],file_config['saved_model_name']), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = pd.read_parquet(file_config.get('test'))\n",
    "# test_r = test_r.rename(columns={'FRS_diag':'code', 'target':'label', 'PID':'patid'})\n",
    "\n",
    "testload = get_loader(test_r)\n",
    "\n",
    "# Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=test_r, max_len=global_params['max_len_seq'],\n",
    "#                                  code='data', label='target')\n",
    "# testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(test_load):\n",
    "        model.eval()\n",
    "        age_ids, input_ids, posi_ids, creatinine, segment_ids, attMask, targets, _ = batch\n",
    "        # targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        creatinine = creatinine.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,creatinine,attention_mask=attMask, labels=targets)\n",
    "        logits = [logit.cpu() for logit in logits]\n",
    "        targets = targets.cpu()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    # y = torch.cat(y, dim=0)\n",
    "\n",
    "    # aps, roc, output, label = precision_test(y, y_label)\n",
    "    return y, y_label\n",
    "\n",
    "\n",
    "\n",
    "y, y_label = evaluation()\n",
    "# aps, roc, output, label = precision_test(y, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 10])"
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label\n",
    "__y =  [torch.cat(_y, dim=1) for _y in y]\n",
    "__y =  torch.cat(__y)\n",
    "__y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3750, 1.0000, 0.5000, 1.2500, 1.2500, 1.3750, 0.8750, 0.8750, 0.8750,\n",
      "        0.6250])\n",
      "tensor([1.3125, 1.0722, 0.6407, 1.1717, 1.2624, 1.4263, 0.8561, 0.7299, 0.8628,\n",
      "        0.7152])\n",
      "tensor([1.6202, 1.2247, 0.8660, 1.4142, 1.5000, 1.5411, 1.1726, 1.0607, 1.0607,\n",
      "        0.9354])\n"
     ]
    }
   ],
   "source": [
    "# (__y * 4 - y_label * 4).abs().mean()\n",
    "\n",
    "# print(((__y * 4).round() - y_label * 4).abs().mean())\n",
    "#\n",
    "print(((__y * 4).round() - y_label * 4).abs().mean(dim=0))\n",
    "print(((__y * 4) - y_label * 4).abs().mean(dim=0))\n",
    "\n",
    "a = (__y * 4).round() - y_label * 4\n",
    "a = a ** 2\n",
    "a = a.mean(dim=0)\n",
    "a = a.sqrt()\n",
    "print(a)\n",
    "a = (__y * 4) - y_label * 4\n",
    "a = a ** 2\n",
    "a = a.mean(dim=0)\n",
    "data = a.sqrt()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5\n",
      "8.002806\n",
      "8.72438\n",
      "9.246621\n"
     ]
    }
   ],
   "source": [
    "a = (__y * 4).round().sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "a = a.abs().mean()\n",
    "print(a.numpy())\n",
    "\n",
    "b = (__y * 4).sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "b = b.abs().mean()\n",
    "print(b.numpy())\n",
    "\n",
    "c = (__y * 4).sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "c = (c ** 2).mean().sqrt()\n",
    "print(c.numpy())\n",
    "\n",
    "d = (__y * 4).round().sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "d = (d **2).mean().sqrt()\n",
    "print(d.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "cols=['Speech', 'Salivation',\n",
    "       'Swallowing', 'Handwriting',\n",
    "       'Cutting', 'Dressing_and_Hygiene',\n",
    "       'Turning_in_Bed', 'Walking',\n",
    "       'Climbing_Stairs', 'Respiratory']\n",
    "res = pd.DataFrame(data=data.view(1,-1).numpy(), columns=cols)\n",
    "mae  = (__y * 4).round().sum(dim=1) - (y_label * 4).sum(dim=1).numpy()\n",
    "res['MAE']=c.numpy()\n",
    "\n",
    "res['n_train'] = n_train\n",
    "res['n_validation'] = n_validation\n",
    "res['n_test'] = n_test\n",
    "res.to_csv(f'next{dt}months_creatinine_small.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "data": {
      "text/plain": "     Speech  Salivation  Swallowing  Handwriting   Cutting  \\\n0  0.840711    0.808245    0.986925     1.052595  1.081831   \n\n   Dressing_and_Hygiene  Turning_in_Bed   Walking  Climbing_Stairs  \\\n0              0.833618        0.884761  0.785584         0.759341   \n\n   Respiratory       MAE  n_train  n_validation  n_test  \n0       0.7131  4.785634      220            55      85  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Speech</th>\n      <th>Salivation</th>\n      <th>Swallowing</th>\n      <th>Handwriting</th>\n      <th>Cutting</th>\n      <th>Dressing_and_Hygiene</th>\n      <th>Turning_in_Bed</th>\n      <th>Walking</th>\n      <th>Climbing_Stairs</th>\n      <th>Respiratory</th>\n      <th>MAE</th>\n      <th>n_train</th>\n      <th>n_validation</th>\n      <th>n_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.840711</td>\n      <td>0.808245</td>\n      <td>0.986925</td>\n      <td>1.052595</td>\n      <td>1.081831</td>\n      <td>0.833618</td>\n      <td>0.884761</td>\n      <td>0.785584</td>\n      <td>0.759341</td>\n      <td>0.7131</td>\n      <td>4.785634</td>\n      <td>220</td>\n      <td>55</td>\n      <td>85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 12 months\n"
     ]
    }
   ],
   "source": [
    "print(f'Done {dt} months')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# (__y * 4 - y_label * 4).abs().mean()\\n\\n# print(((__y * 4).round() - y_label * 4).abs().mean())\\n#\\nprint(((__y * 4).round() - y_label * 4).abs().mean(dim=0))\\nprint(((__y * 4) - y_label * 4).abs().mean(dim=0))\\n\\ntensor([0.3140, 0.5601, 0.3915, 0.4981, 0.4903, 0.4690, 0.5891, 0.4516, 0.5155,\\n        0.5291])\\ntensor([0.4384, 0.5715, 0.5213, 0.5532, 0.5293, 0.5035, 0.5767, 0.4699, 0.5478,\\n        0.5798])\\n\\na = (__y * 4).round().sum(dim=1) - (y_label * 4).sum(dim=1)\\na = a.abs().mean()\\nprint(a)\\n\\nb = (__y * 4).sum(dim=1) - (y_label * 4).sum(dim=1)\\nb = b.abs().mean()\\nprint(b)\\ntensor(2.6182)\\ntensor(2.4791)\\n'"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# (__y * 4 - y_label * 4).abs().mean()\n",
    "\n",
    "# print(((__y * 4).round() - y_label * 4).abs().mean())\n",
    "#\n",
    "print(((__y * 4).round() - y_label * 4).abs().mean(dim=0))\n",
    "print(((__y * 4) - y_label * 4).abs().mean(dim=0))\n",
    "\n",
    "tensor([0.3140, 0.5601, 0.3915, 0.4981, 0.4903, 0.4690, 0.5891, 0.4516, 0.5155,\n",
    "        0.5291])\n",
    "tensor([0.4384, 0.5715, 0.5213, 0.5532, 0.5293, 0.5035, 0.5767, 0.4699, 0.5478,\n",
    "        0.5798])\n",
    "\n",
    "a = (__y * 4).round().sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "a = a.abs().mean()\n",
    "print(a)\n",
    "\n",
    "b = (__y * 4).sum(dim=1) - (y_label * 4).sum(dim=1)\n",
    "b = b.abs().mean()\n",
    "print(b)\n",
    "tensor(2.6182)\n",
    "tensor(2.4791)\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "data": {
      "text/plain": "[WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next12months.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next12months_creatinine.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next12months_creatinine_small.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next3months.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next3months_creatinine.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next3months_creatinine_small.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next6months.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next6months_creatinine.csv'),\n WindowsPath('C:/Users/mannh/Documents/Git/BEHRT_MSC/task/next6months_creatinine_small.csv')]"
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.concat([pd.read_csv(f'next{m}months_creatinine.csv') for m in (3,6,12)])\n",
    "# df = df.rename(columns={df.columns[0]:'nextXmonths'})\n",
    "# df['nextXmonths'] = (3,6,12)\n",
    "# df['lab_results'] = ['None']*3\n",
    "# df = df.set_index('lab_results')\n",
    "# df.to_csv('results.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 Speech  Salivation  Swallowing  Handwriting  \\\nfile                                                                           \nnext12months                   1.569659    1.094108    0.611420     1.370049   \nnext12months_creatinine        1.667354    1.215627    0.685642     1.299823   \nnext12months_creatinine_small  1.623852    1.323368    0.744738     1.352167   \nnext3months                    0.587059    0.666650    0.664830     0.864030   \nnext3months_creatinine         0.631635    0.706722    0.680364     0.853704   \nnext3months_creatinine_small   0.616508    0.690694    0.681805     0.850846   \nnext6months                    0.775040    0.759797    0.940897     1.088254   \nnext6months_creatinine         0.787176    0.814266    0.955292     1.049367   \nnext6months_creatinine_small   0.840711    0.808245    0.986924     1.052595   \n\n                                Cutting  Dressing_and_Hygiene  Turning_in_Bed  \\\nfile                                                                            \nnext12months                   1.313293              1.572980        0.899035   \nnext12months_creatinine        1.004725              1.379895        0.609507   \nnext12months_creatinine_small  1.464452              1.626652        0.975365   \nnext3months                    0.770703              0.698961        0.768706   \nnext3months_creatinine         0.760213              0.695057        0.770713   \nnext3months_creatinine_small   0.763858              0.696371        0.786722   \nnext6months                    1.087134              0.851184        0.873855   \nnext6months_creatinine         1.062354              0.851369        0.897804   \nnext6months_creatinine_small   1.081831              0.833618        0.884761   \n\n                                Walking  Climbing_Stairs  Respiratory  \\\nfile                                                                    \nnext12months                   0.782292         1.162184     1.031419   \nnext12months_creatinine        0.670426         0.988132     1.007476   \nnext12months_creatinine_small  0.854922         1.012737     0.960745   \nnext3months                    0.611158         0.726082     0.751973   \nnext3months_creatinine         0.600466         0.721354     0.751448   \nnext3months_creatinine_small   0.600480         0.736056     0.786579   \nnext6months                    0.749641         0.775839     0.680189   \nnext6months_creatinine         0.782094         0.776430     0.662205   \nnext6months_creatinine_small   0.785584         0.759341     0.713100   \n\n                                    MAE  n_train  n_validation  n_test  \nfile                                                                    \nnext12months                   7.410675       60            15       8  \nnext12months_creatinine        6.612772       60            15       8  \nnext12months_creatinine_small  8.724380       60            15       8  \nnext3months                    3.813820      779           195     234  \nnext3months_creatinine         3.790790      779           195     234  \nnext3months_creatinine_small   3.879244      779           195     234  \nnext6months                    4.720744      779           195     234  \nnext6months_creatinine         4.644844      779           195     234  \nnext6months_creatinine_small   4.785634      220            55      85  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Speech</th>\n      <th>Salivation</th>\n      <th>Swallowing</th>\n      <th>Handwriting</th>\n      <th>Cutting</th>\n      <th>Dressing_and_Hygiene</th>\n      <th>Turning_in_Bed</th>\n      <th>Walking</th>\n      <th>Climbing_Stairs</th>\n      <th>Respiratory</th>\n      <th>MAE</th>\n      <th>n_train</th>\n      <th>n_validation</th>\n      <th>n_test</th>\n    </tr>\n    <tr>\n      <th>file</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>next12months</th>\n      <td>1.569659</td>\n      <td>1.094108</td>\n      <td>0.611420</td>\n      <td>1.370049</td>\n      <td>1.313293</td>\n      <td>1.572980</td>\n      <td>0.899035</td>\n      <td>0.782292</td>\n      <td>1.162184</td>\n      <td>1.031419</td>\n      <td>7.410675</td>\n      <td>60</td>\n      <td>15</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>next12months_creatinine</th>\n      <td>1.667354</td>\n      <td>1.215627</td>\n      <td>0.685642</td>\n      <td>1.299823</td>\n      <td>1.004725</td>\n      <td>1.379895</td>\n      <td>0.609507</td>\n      <td>0.670426</td>\n      <td>0.988132</td>\n      <td>1.007476</td>\n      <td>6.612772</td>\n      <td>60</td>\n      <td>15</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>next12months_creatinine_small</th>\n      <td>1.623852</td>\n      <td>1.323368</td>\n      <td>0.744738</td>\n      <td>1.352167</td>\n      <td>1.464452</td>\n      <td>1.626652</td>\n      <td>0.975365</td>\n      <td>0.854922</td>\n      <td>1.012737</td>\n      <td>0.960745</td>\n      <td>8.724380</td>\n      <td>60</td>\n      <td>15</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>next3months</th>\n      <td>0.587059</td>\n      <td>0.666650</td>\n      <td>0.664830</td>\n      <td>0.864030</td>\n      <td>0.770703</td>\n      <td>0.698961</td>\n      <td>0.768706</td>\n      <td>0.611158</td>\n      <td>0.726082</td>\n      <td>0.751973</td>\n      <td>3.813820</td>\n      <td>779</td>\n      <td>195</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>next3months_creatinine</th>\n      <td>0.631635</td>\n      <td>0.706722</td>\n      <td>0.680364</td>\n      <td>0.853704</td>\n      <td>0.760213</td>\n      <td>0.695057</td>\n      <td>0.770713</td>\n      <td>0.600466</td>\n      <td>0.721354</td>\n      <td>0.751448</td>\n      <td>3.790790</td>\n      <td>779</td>\n      <td>195</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>next3months_creatinine_small</th>\n      <td>0.616508</td>\n      <td>0.690694</td>\n      <td>0.681805</td>\n      <td>0.850846</td>\n      <td>0.763858</td>\n      <td>0.696371</td>\n      <td>0.786722</td>\n      <td>0.600480</td>\n      <td>0.736056</td>\n      <td>0.786579</td>\n      <td>3.879244</td>\n      <td>779</td>\n      <td>195</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>next6months</th>\n      <td>0.775040</td>\n      <td>0.759797</td>\n      <td>0.940897</td>\n      <td>1.088254</td>\n      <td>1.087134</td>\n      <td>0.851184</td>\n      <td>0.873855</td>\n      <td>0.749641</td>\n      <td>0.775839</td>\n      <td>0.680189</td>\n      <td>4.720744</td>\n      <td>779</td>\n      <td>195</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>next6months_creatinine</th>\n      <td>0.787176</td>\n      <td>0.814266</td>\n      <td>0.955292</td>\n      <td>1.049367</td>\n      <td>1.062354</td>\n      <td>0.851369</td>\n      <td>0.897804</td>\n      <td>0.782094</td>\n      <td>0.776430</td>\n      <td>0.662205</td>\n      <td>4.644844</td>\n      <td>779</td>\n      <td>195</td>\n      <td>234</td>\n    </tr>\n    <tr>\n      <th>next6months_creatinine_small</th>\n      <td>0.840711</td>\n      <td>0.808245</td>\n      <td>0.986924</td>\n      <td>1.052595</td>\n      <td>1.081831</td>\n      <td>0.833618</td>\n      <td>0.884761</td>\n      <td>0.785584</td>\n      <td>0.759341</td>\n      <td>0.713100</td>\n      <td>4.785634</td>\n      <td>220</td>\n      <td>55</td>\n      <td>85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "t_df = pd.DataFrame()\n",
    "for file in Path(os.getcwd()).rglob('next*months*.csv'):\n",
    "    t_df = pd.concat([t_df, pd.read_csv(file).assign(file=file.name.replace('.csv','')).iloc[:,1:]])\n",
    "t_df = t_df.set_index('file')\n",
    "t_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "t_df.to_csv('models_summary.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "behrt_msc",
   "language": "python",
   "display_name": "BEHRT_MSC"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}